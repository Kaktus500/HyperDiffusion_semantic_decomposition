{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pymeshlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chair_dir = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/data/chair/3068\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(chair_dir / \"result_after_merging.json\") as f:\n",
    "    meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = pymeshlab.MeshSet()\n",
    "ms.load_new_mesh(str(chair_dir / \"objs\" / \"original-1.obj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as h5_file:\n",
    "        # List all groups\n",
    "        print(\"Keys: %s\" % h5_file.keys())\n",
    "        # Get the data\n",
    "        data = {}\n",
    "        for key in h5_file.keys():\n",
    "            data[key] = h5_file[key][()]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = Path(\"/home/pauldelseith/dataset_storage/partnet/data_v0/\")\n",
    "model_class_anno_id_mapping = defaultdict(list)\n",
    "for folder in parent_dir.iterdir():\n",
    "    if folder.is_dir():\n",
    "        with open(folder / \"meta.json\", \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        model_class_anno_id_mapping[meta[\"model_cat\"]].append(meta[\"anno_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(\"partnet_class_mapping.json\"), \"w\") as f:\n",
    "    json.dump(model_class_anno_id_mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_seg_path = Path(\"/home/pauldelseith/dataset_storage/partnet/sem_seg_h5/Chair-1/train-00.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_seg_path = Path(\"/home/pauldelseith/dataset_storage/partnet/ins_seg_h5/Chair/train-00.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenet_seg = Path(\"/home/pauldelseith/dataset_storage/shapenet_seg/hdf5_data/ply_data_train0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_seg_annotations = load_h5_file(sem_seg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_seg_annotations = load_h5_file(ins_seg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenet_seg_h5 = load_h5_file(shapenet_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_seg_annotations[\"nor\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_seg_annotations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenet_seg_h5.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenet_seg_h5[\"pid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_seg_annotations[\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = trimesh.points.PointCloud(sem_seg_annotations[\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_convex_hull = pc.convex_hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_seg_annotations[\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_seg_points = sem_seg_annotations[\"data\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_file_path = \"/home/pauldelseith/HyperDiffusion_semantic_decomposition/data/partnet_100000_pc_occ_in_out_True/new-1_manifold.obj.npy\"\n",
    "point_cloud_data = np.load(npy_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_data\n",
    "point_cloud_data.shape\n",
    "x, y, z, intensity = point_cloud_data[point_cloud_data[:,3] == 1].T\n",
    "colors = intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(x, y, z, c=colors, cmap='viridis', marker='o')\n",
    "scatter.axes.set_xlim3d([-1,1]) \n",
    "scatter.axes.set_ylim3d([-1,1]) \n",
    "scatter.axes.set_zlim3d([-1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.load(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/data/partnet/new-1_manifold.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.vertices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.vertices.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_data = mesh.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_surface = mesh.sample(100000) + 0.01 * np.random.randn(100000, 3)\n",
    "pc_export = trimesh.PointCloud(points_surface).export(\"pc1.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "def visualize_npy_pointcloud(file_path):\n",
    "    # Load the point cloud data from the .npy file\n",
    "    point_cloud_data = np.load(file_path)\n",
    "    point_cloud_data = point_cloud_data[point_cloud_data[:,3] == 1] # filter 1s\n",
    "    # Create two Open3D PointCloud objects for different markers\n",
    "    pcd_0 = o3d.geometry.PointCloud()\n",
    "    pcd_1 = o3d.geometry.PointCloud()\n",
    "    # Separate points based on the fourth parameter\n",
    "    mask_0 = point_cloud_data[:, 3] == 0\n",
    "    mask_1 = point_cloud_data[:, 3] == 1\n",
    "    points_0 = point_cloud_data[mask_0][:, :3]\n",
    "    points_1 = point_cloud_data[mask_1][:, :3]\n",
    "    # Assign the numpy point cloud data to the Open3D PointClouds\n",
    "    pcd_0.points = o3d.utility.Vector3dVector(points_0)\n",
    "    pcd_1.points = o3d.utility.Vector3dVector(points_1)\n",
    "    # If the point cloud has colors (assuming RGB format with values 0-255)\n",
    "    if point_cloud_data.shape[1] == 7:\n",
    "        colors_0 = point_cloud_data[mask_0][:, 4:7] / 255.0\n",
    "        colors_1 = point_cloud_data[mask_1][:, 4:7] / 255.0\n",
    "        pcd_0.colors = o3d.utility.Vector3dVector(colors_0)\n",
    "        pcd_1.colors = o3d.utility.Vector3dVector(colors_1)\n",
    "    else:\n",
    "        # Assign default colors\n",
    "        pcd_0.paint_uniform_color([1, 0, 0])  # Red for points with 4th parameter 0\n",
    "        pcd_1.paint_uniform_color([0, 1, 0])  # Green for points with 4th parameter 1\n",
    "    # Visualize the point clouds\n",
    "    o3d.visualization.draw_plotly([pcd_0, pcd_1])\n",
    "# Example usage\n",
    "visualize_npy_pointcloud(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/data/partnet_100000_pc_occ_in_out_True/new-11_manifold.obj.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_data = ins_seg_annotations[\"pts\"]\n",
    "#point_cloud_data = np.concatenate((point_cloud_data, sem_seg_annotations[\"label_seg\"][:,:,np.newaxis]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sem_seg_annotations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_seg_annotations[\"label_seg\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_seg_annotations[\"label_seg\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z, intensity = point_cloud_data[0].T\n",
    "colors = intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(x, y, z, c=colors, cmap='viridis', marker='o')\n",
    "scatter.axes.set_xlim3d([-1,1]) \n",
    "scatter.axes.set_ylim3d([-1,1]) \n",
    "scatter.axes.set_zlim3d([-1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points_uniform = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_uniform = np.random.uniform(\n",
    "                    -1.0, 1.0, size=(n_points_uniform, 3)\n",
    "                )\n",
    "points_surface = point_cloud_data[0][:,:3]\n",
    "points_surface += 0.01 * np.random.randn(points_surface.shape[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.concatenate([points_surface, points_uniform], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
