{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_check_metrics_file = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/siren/experiment_scripts/logs/chair_complete_split_merged_ply/metrics_new.json\")\n",
    "with open(quality_check_metrics_file, \"r\") as f:\n",
    "    quality_check_metrics = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_difference_fractions = []\n",
    "filled_difference_fractions_full = []\n",
    "for value in quality_check_metrics.values():\n",
    "    # filled_difference_fractions.append(value[\"split\"][\"filled_difference_fraction\"])\n",
    "    filled_difference_fractions_full.append(value[\"full\"][\"filled_difference_fraction\"])\n",
    "# filled_difference_fractions = np.array(filled_difference_fractions)\n",
    "filled_difference_fractions_full = np.array(filled_difference_fractions_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_difference_fractions.mean()\n",
    "np.percentile(filled_difference_fractions, [30, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_difference_fractions_full.mean()\n",
    "np.percentile(filled_difference_fractions_full, [30, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter mlps according to filled_difference_fraction\n",
    "filled_fraction_threshold = 0.1\n",
    "mlps_to_keep = []\n",
    "for key, value in quality_check_metrics.items():\n",
    "    if value[\"full\"][\"filled_difference_fraction\"] < filled_fraction_threshold:\n",
    "        mlps_to_keep.append(key)\n",
    "print(f\"Keeping {len(mlps_to_keep)} out of {len(quality_check_metrics)} MLPs\")\n",
    "good_mlps_list_path = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/siren/experiment_scripts/logs/chair_complete_split_merged/good_shapes.lst\")\n",
    "with open(good_mlps_list_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(mlps_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_weights_path = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/mlp_weights/chair_complete_split_merged\")\n",
    "train_logs_path = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/siren/experiment_scripts/logs/chair_complete_split_merged\")\n",
    "for mlp_weight in train_logs_path.glob(\"*.pth\"):\n",
    "    shape_id = mlp_weight.stem.split(\"_\")[1]\n",
    "    if shape_id in mlps_to_keep:\n",
    "        shutil.copy(mlp_weight, mlp_weights_path / mlp_weight.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshes_path = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/data/partnet/sem_seg_meshes/Chair\")\n",
    "meshes = []\n",
    "for mesh in meshes_path.glob(\"*.obj\"):\n",
    "    shape_id = mesh.stem.split(\"_\")[0]\n",
    "    if shape_id in mlps_to_keep:\n",
    "        meshes.append(mesh)\n",
    "with open(meshes_path / \"hyper_diff_complete_meshes.lst\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([mesh.name for mesh in meshes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlps_trained_list_path = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/siren/experiment_scripts/logs/chair_base_seat_sorted_classes/mlp_list.lst\")\n",
    "with open(mlps_trained_list_path, \"r\") as f:\n",
    "    mlps_trained_list = f.readlines()\n",
    "shapes_trained_list = []\n",
    "for line in mlps_trained_list:\n",
    "    shapes_trained_list.append(line.split(\"_\")[1]) \n",
    "with open(mlps_trained_list_path.parent / \"shapes_trained.lst\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(shapes_trained_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate weight files into single folder\n",
    "base_path = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/siren/experiment_scripts/logs\")\n",
    "files = [\"chair_complete_trial\", \"5\", \"6\", \"7\", \"8\"]\n",
    "merged_dir = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/siren/experiment_scripts/logs/chair_complete_split_merged\")\n",
    "merged_dir.mkdir(exist_ok=True)\n",
    "shape_ids = set()\n",
    "for file in files:\n",
    "    weight_directory = base_path / f\"{file}\"\n",
    "    for weight_file in weight_directory.glob(\"*.pth\"):\n",
    "        shape_id = weight_file.stem.split(\"_\")[1]\n",
    "        if shape_id in shape_ids:\n",
    "            continue\n",
    "        shutil.copy(weight_file, merged_dir / weight_file.name)\n",
    "        shape_ids.add(shape_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_ids_unique = set(shape_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shape_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shape_ids_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\n",
    "    \"/home/pauldelseith/HyperDiffusion_semantic_decomposition\"\n",
    ")\n",
    "from progressbar import ProgressBar\n",
    "from siren.sdf_visualization import generate_mesh_from_sdf\n",
    "from hydra import compose, initialize\n",
    "generated_meshes_dir = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/siren/experiment_scripts/logs/chair_complete_split_merged_ply\")\n",
    "generated_meshes_shapes = [file.name.split(\"_\")[1] for file in generated_meshes_dir.glob(\"*.ply\")]\n",
    "generated_meshes_shapes = set(generated_meshes_shapes)\n",
    "config_path_relative = Path(\"../configs/overfitting_configs\")\n",
    "with initialize(version_base=None, config_path=str(config_path_relative)):\n",
    "        cfg = compose(config_name=\"overfit_chair\")\n",
    "progress_bar = ProgressBar(\n",
    "        maxval=len(list(merged_dir.glob(\"*\")))\n",
    "    ).start()\n",
    "shapes_skipped = 0\n",
    "for shape_id in shape_ids:\n",
    "    if shape_id in generated_meshes_shapes:\n",
    "        progress_bar.update(progress_bar.currval + 1)\n",
    "        shapes_skipped += 1\n",
    "        if shapes_skipped % 100 == 0:\n",
    "            print(f\"Skipped {shapes_skipped} shapes\")\n",
    "        continue\n",
    "    file = list(merged_dir.glob(f\"*{shape_id}*.pth\"))[0]\n",
    "    generate_mesh_from_sdf(file, generated_meshes_dir / file.stem, cfg, split_sdf=True)\n",
    "    progress_bar.update(progress_bar.currval + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of shapes that still have to be trained\n",
    "mlp_weights_path = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/siren/experiment_scripts/logs/chair_complete_baseline\")\n",
    "shapes_trained = [file.stem.split(\"_\")[1] for file in mlp_weights_path.glob(\"*.pth\")]\n",
    "train_mesh_dir = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/data/partnet/sem_seg_meshes/\")\n",
    "splits = [\"Chair_100000_pc_occ_in_out_True_full_split_5\", \"Chair_100000_pc_occ_in_out_True_full_split_6\"]\n",
    "shapes_to_train = []\n",
    "for split in splits:\n",
    "    meshes = list((train_mesh_dir / split).glob(\"*.npy\"))\n",
    "    shape_ids = [mesh.stem.split(\"_\")[0] for mesh in meshes]\n",
    "    shape_ids = set(shape_ids)\n",
    "    shapes_to_train.append(list(shape_ids - set(shapes_trained)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, split in enumerate(splits):\n",
    "    with open(train_mesh_dir / split / \"shapes_to_train.lst\", \"w\") as f:\n",
    "        for shape_id in shapes_to_train[idx]:\n",
    "            f.write(str(shape_id) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_meshes_dir = Path(\"/home/pauldelseith/HyperDiffusion_semantic_decomposition/siren/experiment_scripts/logs/chair_complete_split_merged_ply\")\n",
    "generated_meshes_shapes = [file.name.split(\"_\")[1] for file in generated_meshes_dir.glob(\"*.ply\")]\n",
    "generated_meshes_shapes = set(generated_meshes_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generated_meshes_shapes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
